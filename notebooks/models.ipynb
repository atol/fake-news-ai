{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>claim</th>\n",
       "      <th>claimant</th>\n",
       "      <th>date</th>\n",
       "      <th>label</th>\n",
       "      <th>related_articles</th>\n",
       "      <th>id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>9389</td>\n",
       "      <td>While arguing over President Reagan’s 1981 tax...</td>\n",
       "      <td>Sarah Sanders</td>\n",
       "      <td>2017-10-31</td>\n",
       "      <td>1</td>\n",
       "      <td>[34218, 55700, 18736, 39031, 34219, 34220]</td>\n",
       "      <td>10354</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1861</td>\n",
       "      <td>Recently Rick Scott \"closed 30 women’s health ...</td>\n",
       "      <td>Lois Frankel</td>\n",
       "      <td>2014-09-12</td>\n",
       "      <td>0</td>\n",
       "      <td>[73190, 76997, 38841, 77415, 77303, 9280, 8332...</td>\n",
       "      <td>2053</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11035</td>\n",
       "      <td>Says Target installed urinals in a women’s bat...</td>\n",
       "      <td>Facebook posts</td>\n",
       "      <td>2016-04-22</td>\n",
       "      <td>0</td>\n",
       "      <td>[9619, 22197]</td>\n",
       "      <td>12160</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12221</td>\n",
       "      <td>Says \"combined doses of vaccines\" have never b...</td>\n",
       "      <td>Facebook posts</td>\n",
       "      <td>2019-04-15</td>\n",
       "      <td>0</td>\n",
       "      <td>[57163, 31528, 40908, 31536, 68904, 44601]</td>\n",
       "      <td>13458</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11354</td>\n",
       "      <td>:   The AMBER Alert system has been discontinu...</td>\n",
       "      <td></td>\n",
       "      <td>2013-10-13</td>\n",
       "      <td>0</td>\n",
       "      <td>[103978, 121475, 121849]</td>\n",
       "      <td>12504</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2910</td>\n",
       "      <td>Health insurance costs for Floridians are up 3...</td>\n",
       "      <td>Republican Party of Florida</td>\n",
       "      <td>2014-09-23</td>\n",
       "      <td>1</td>\n",
       "      <td>[9581, 89571, 7836, 7945, 7949, 77360, 83491, ...</td>\n",
       "      <td>3208</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6096</td>\n",
       "      <td>A photograph captures Harriet Tubman as a \"Gun...</td>\n",
       "      <td></td>\n",
       "      <td>2019-03-25</td>\n",
       "      <td>0</td>\n",
       "      <td>[125108, 125968, 126005]</td>\n",
       "      <td>6701</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10446</td>\n",
       "      <td>ISIS leader Abu Bakr al Baghdadi was \"released...</td>\n",
       "      <td>Jeanine  Pirro</td>\n",
       "      <td>2014-06-14</td>\n",
       "      <td>0</td>\n",
       "      <td>[80115, 93998, 5968, 175, 91475, 8710, 89881, ...</td>\n",
       "      <td>11514</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5414</td>\n",
       "      <td>\"The board of a nonprofit organization on whic...</td>\n",
       "      <td>Tennessee Republican Party</td>\n",
       "      <td>2008-02-25</td>\n",
       "      <td>1</td>\n",
       "      <td>[96453, 71123, 61, 69968, 96477]</td>\n",
       "      <td>5966</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6657</td>\n",
       "      <td>Japan announced plans to dump 920,000 tons of ...</td>\n",
       "      <td></td>\n",
       "      <td>2017-08-22</td>\n",
       "      <td>1</td>\n",
       "      <td>[120293, 132262]</td>\n",
       "      <td>7328</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>12444 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   claim  \\\n",
       "9389   While arguing over President Reagan’s 1981 tax...   \n",
       "1861   Recently Rick Scott \"closed 30 women’s health ...   \n",
       "11035  Says Target installed urinals in a women’s bat...   \n",
       "12221  Says \"combined doses of vaccines\" have never b...   \n",
       "11354  :   The AMBER Alert system has been discontinu...   \n",
       "...                                                  ...   \n",
       "2910   Health insurance costs for Floridians are up 3...   \n",
       "6096   A photograph captures Harriet Tubman as a \"Gun...   \n",
       "10446  ISIS leader Abu Bakr al Baghdadi was \"released...   \n",
       "5414   \"The board of a nonprofit organization on whic...   \n",
       "6657   Japan announced plans to dump 920,000 tons of ...   \n",
       "\n",
       "                          claimant        date  label  \\\n",
       "9389                 Sarah Sanders  2017-10-31      1   \n",
       "1861                  Lois Frankel  2014-09-12      0   \n",
       "11035               Facebook posts  2016-04-22      0   \n",
       "12221               Facebook posts  2019-04-15      0   \n",
       "11354                               2013-10-13      0   \n",
       "...                            ...         ...    ...   \n",
       "2910   Republican Party of Florida  2014-09-23      1   \n",
       "6096                                2019-03-25      0   \n",
       "10446               Jeanine  Pirro  2014-06-14      0   \n",
       "5414    Tennessee Republican Party  2008-02-25      1   \n",
       "6657                                2017-08-22      1   \n",
       "\n",
       "                                        related_articles     id  \n",
       "9389          [34218, 55700, 18736, 39031, 34219, 34220]  10354  \n",
       "1861   [73190, 76997, 38841, 77415, 77303, 9280, 8332...   2053  \n",
       "11035                                      [9619, 22197]  12160  \n",
       "12221         [57163, 31528, 40908, 31536, 68904, 44601]  13458  \n",
       "11354                           [103978, 121475, 121849]  12504  \n",
       "...                                                  ...    ...  \n",
       "2910   [9581, 89571, 7836, 7945, 7949, 77360, 83491, ...   3208  \n",
       "6096                            [125108, 125968, 126005]   6701  \n",
       "10446  [80115, 93998, 5968, 175, 91475, 8710, 89881, ...  11514  \n",
       "5414                    [96453, 71123, 61, 69968, 96477]   5966  \n",
       "6657                                    [120293, 132262]   7328  \n",
       "\n",
       "[12444 rows x 6 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_set = pd.read_pickle('train_set.pkl')\n",
    "train_set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_set = pd.read_pickle('test_set.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import sklearn\n",
    "from sklearn.metrics import accuracy_score, f1_score, confusion_matrix\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.model_selection import ShuffleSplit\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from imblearn.pipeline import Pipeline\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from imblearn.over_sampling import SMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_claims = train_set.claim\n",
    "y_train_claims = train_set.label\n",
    "X_test_claims = test_set.claim\n",
    "y_test_claims = test_set.label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "sm = SMOTE()\n",
    "cv = CountVectorizer()\n",
    "nb = MultinomialNB()\n",
    "\n",
    "pipeline = Pipeline([('cv', cv),('sm', sm), ('nb', nb)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "         steps=[('cv',\n",
       "                 CountVectorizer(analyzer='word', binary=False,\n",
       "                                 decode_error='strict',\n",
       "                                 dtype=<class 'numpy.int64'>, encoding='utf-8',\n",
       "                                 input='content', lowercase=True, max_df=1.0,\n",
       "                                 max_features=None, min_df=1,\n",
       "                                 ngram_range=(1, 1), preprocessor=None,\n",
       "                                 stop_words=None, strip_accents=None,\n",
       "                                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                 tokenizer=None, vocabulary=None)),\n",
       "                ('sm',\n",
       "                 SMOTE(k_neighbors=5, kind='deprecated',\n",
       "                       m_neighbors='deprecated', n_jobs=1,\n",
       "                       out_step='deprecated', random_state=None, ratio=None,\n",
       "                       sampling_strategy='auto', svm_estimator='deprecated')),\n",
       "                ('nb',\n",
       "                 MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True))],\n",
       "         verbose=False)"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline.fit(X_train_claims, y_train_claims)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_claims = pipeline.predict(X_test_claims)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 58.37%\n",
      "\n",
      "F1 Score: 48.10\n",
      "\n",
      "Confusion Matrix:\n",
      " [[837 512 105]\n",
      " [336 923  75]\n",
      " [ 97 170  56]]\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy: {:.2f}%\".format(accuracy_score(y_test_claims, y_pred_claims) * 100))\n",
    "print(\"\\nF1 Score: {:.2f}\".format(f1_score(y_test_claims, y_pred_claims, average='macro') * 100))\n",
    "print(\"\\nConfusion Matrix:\\n\", confusion_matrix(y_test_claims, y_pred_claims))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "tv = TfidfVectorizer()\n",
    "\n",
    "pipeline2 = Pipeline([('tv', tv),('sm', sm), ('nb', nb)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "         steps=[('tv',\n",
       "                 TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                 decode_error='strict',\n",
       "                                 dtype=<class 'numpy.float64'>,\n",
       "                                 encoding='utf-8', input='content',\n",
       "                                 lowercase=True, max_df=1.0, max_features=None,\n",
       "                                 min_df=1, ngram_range=(1, 1), norm='l2',\n",
       "                                 preprocessor=None, smooth_idf=True,\n",
       "                                 stop_words=None, strip_accents=None,\n",
       "                                 sublinear_tf=False,\n",
       "                                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                 tokenizer=None, use_idf=True,\n",
       "                                 vocabulary=None)),\n",
       "                ('sm',\n",
       "                 SMOTE(k_neighbors=5, kind='deprecated',\n",
       "                       m_neighbors='deprecated', n_jobs=1,\n",
       "                       out_step='deprecated', random_state=None, ratio=None,\n",
       "                       sampling_strategy='auto', svm_estimator='deprecated')),\n",
       "                ('nb',\n",
       "                 MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True))],\n",
       "         verbose=False)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline2.fit(X_train_claims, y_train_claims)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_claims = pipeline2.predict(X_test_claims)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 54.23%\n",
      "\n",
      "F1 Score: 46.55\n",
      "\n",
      "Confusion Matrix:\n",
      " [[801 461 192]\n",
      " [288 799 247]\n",
      " [ 92 144  87]]\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy: {:.2f}%\".format(accuracy_score(y_test_claims, y_pred_claims) * 100))\n",
    "print(\"\\nF1 Score: {:.2f}\".format(f1_score(y_test_claims, y_pred_claims, average='macro') * 100))\n",
    "print(\"\\nConfusion Matrix:\\n\", confusion_matrix(y_test_claims, y_pred_claims))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../models/train_claims_nb.pkl\", 'wb') as f:\n",
    "    pickle.dump(pipeline, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_claimants = train_set.claimant\n",
    "y_train_claimants = train_set.label\n",
    "X_test_claimants = test_set.claimant\n",
    "y_test_claimants = test_set.label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline3 = Pipeline([('vc', vc),('sm', sm), ('nb', nb)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "         steps=[('vc',\n",
       "                 CountVectorizer(analyzer='word', binary=False,\n",
       "                                 decode_error='strict',\n",
       "                                 dtype=<class 'numpy.int64'>, encoding='utf-8',\n",
       "                                 input='content', lowercase=True, max_df=1.0,\n",
       "                                 max_features=None, min_df=1,\n",
       "                                 ngram_range=(1, 1), preprocessor=None,\n",
       "                                 stop_words=None, strip_accents=None,\n",
       "                                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                 tokenizer=None, vocabulary=None)),\n",
       "                ('sm',\n",
       "                 SMOTE(k_neighbors=5, kind='deprecated',\n",
       "                       m_neighbors='deprecated', n_jobs=1,\n",
       "                       out_step='deprecated', random_state=None, ratio=None,\n",
       "                       sampling_strategy='auto', svm_estimator='deprecated')),\n",
       "                ('nb',\n",
       "                 MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True))],\n",
       "         verbose=False)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline3.fit(X_train_claimants, y_train_claimants)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_claimants = pipeline3.predict(X_test_claimants)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 53.87%\n",
      "\n",
      "F1 Score: 43.90\n",
      "\n",
      "Confusion Matrix:\n",
      " [[1135  201  118]\n",
      " [ 540  464  330]\n",
      " [ 158   88   77]]\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy: {:.2f}%\".format(accuracy_score(y_test_claimants, y_pred_claimants) * 100))\n",
    "print(\"\\nF1 Score: {:.2f}\".format(f1_score(y_test_claimants, y_pred_claimants, average='macro') * 100))\n",
    "print(\"\\nConfusion Matrix:\\n\", confusion_matrix(y_test_claimants, y_pred_claimants))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline4 = Pipeline([('tv', tv),('sm', sm), ('nb', nb)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "         steps=[('tv',\n",
       "                 TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                 decode_error='strict',\n",
       "                                 dtype=<class 'numpy.float64'>,\n",
       "                                 encoding='utf-8', input='content',\n",
       "                                 lowercase=True, max_df=1.0, max_features=None,\n",
       "                                 min_df=1, ngram_range=(1, 1), norm='l2',\n",
       "                                 preprocessor=None, smooth_idf=True,\n",
       "                                 stop_words=None, strip_accents=None,\n",
       "                                 sublinear_tf=False,\n",
       "                                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                 tokenizer=None, use_idf=True,\n",
       "                                 vocabulary=None)),\n",
       "                ('sm',\n",
       "                 SMOTE(k_neighbors=5, kind='deprecated',\n",
       "                       m_neighbors='deprecated', n_jobs=1,\n",
       "                       out_step='deprecated', random_state=None, ratio=None,\n",
       "                       sampling_strategy='auto', svm_estimator='deprecated')),\n",
       "                ('nb',\n",
       "                 MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True))],\n",
       "         verbose=False)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline4.fit(X_train_claimants, y_train_claimants)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_claimants = pipeline4.predict(X_test_claimants)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 54.19%\n",
      "\n",
      "F1 Score: 44.53\n",
      "\n",
      "Confusion Matrix:\n",
      " [[1131  202  121]\n",
      " [ 523  472  339]\n",
      " [ 157   83   83]]\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy: {:.2f}%\".format(accuracy_score(y_test_claimants, y_pred_claimants) * 100))\n",
    "print(\"\\nF1 Score: {:.2f}\".format(f1_score(y_test_claimants, y_pred_claimants, average='macro') * 100))\n",
    "print(\"\\nConfusion Matrix:\\n\", confusion_matrix(y_test_claimants, y_pred_claimants))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "svc = LinearSVC()\n",
    "\n",
    "pipeline5 = Pipeline([('tv', tv),('sm', sm), ('svc', svc)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "         steps=[('tv',\n",
       "                 TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                 decode_error='strict',\n",
       "                                 dtype=<class 'numpy.float64'>,\n",
       "                                 encoding='utf-8', input='content',\n",
       "                                 lowercase=True, max_df=1.0, max_features=None,\n",
       "                                 min_df=1, ngram_range=(1, 1), norm='l2',\n",
       "                                 preprocessor=None, smooth_idf=True,\n",
       "                                 stop_words=None, strip_accents=None,\n",
       "                                 sublinear_tf=False,\n",
       "                                 token_pattern='(?u...\n",
       "                       m_neighbors='deprecated', n_jobs=1,\n",
       "                       out_step='deprecated', random_state=None, ratio=None,\n",
       "                       sampling_strategy='auto', svm_estimator='deprecated')),\n",
       "                ('svc',\n",
       "                 LinearSVC(C=1.0, class_weight=None, dual=True,\n",
       "                           fit_intercept=True, intercept_scaling=1,\n",
       "                           loss='squared_hinge', max_iter=1000,\n",
       "                           multi_class='ovr', penalty='l2', random_state=None,\n",
       "                           tol=0.0001, verbose=0))],\n",
       "         verbose=False)"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline5.fit(X_train_claimants, y_train_claimants)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_claimants = pipeline5.predict(X_test_claimants)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 41.40%\n",
      "\n",
      "F1 Score: 40.18\n",
      "\n",
      "Confusion Matrix:\n",
      " [[513 236 705]\n",
      " [219 570 545]\n",
      " [ 30  88 205]]\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy: {:.2f}%\".format(accuracy_score(y_test_claimants, y_pred_claimants) * 100))\n",
    "print(\"\\nF1 Score: {:.2f}\".format(f1_score(y_test_claimants, y_pred_claimants, average='macro') * 100))\n",
    "print(\"\\nConfusion Matrix:\\n\", confusion_matrix(y_test_claimants, y_pred_claimants))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline6 = Pipeline([('cv', cv),('sm', sm), ('svc', svc)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "         steps=[('cv',\n",
       "                 CountVectorizer(analyzer='word', binary=False,\n",
       "                                 decode_error='strict',\n",
       "                                 dtype=<class 'numpy.int64'>, encoding='utf-8',\n",
       "                                 input='content', lowercase=True, max_df=1.0,\n",
       "                                 max_features=None, min_df=1,\n",
       "                                 ngram_range=(1, 1), preprocessor=None,\n",
       "                                 stop_words=None, strip_accents=None,\n",
       "                                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                 tokenizer=None, vocabulary=None)...\n",
       "                       m_neighbors='deprecated', n_jobs=1,\n",
       "                       out_step='deprecated', random_state=None, ratio=None,\n",
       "                       sampling_strategy='auto', svm_estimator='deprecated')),\n",
       "                ('svc',\n",
       "                 LinearSVC(C=1.0, class_weight=None, dual=True,\n",
       "                           fit_intercept=True, intercept_scaling=1,\n",
       "                           loss='squared_hinge', max_iter=1000,\n",
       "                           multi_class='ovr', penalty='l2', random_state=None,\n",
       "                           tol=0.0001, verbose=0))],\n",
       "         verbose=False)"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline6.fit(X_train_claimants, y_train_claimants)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_claimants = pipeline6.predict(X_test_claimants)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 41.59%\n",
      "\n",
      "F1 Score: 39.97\n",
      "\n",
      "Confusion Matrix:\n",
      " [[517 245 692]\n",
      " [239 591 504]\n",
      " [ 33 104 186]]\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy: {:.2f}%\".format(accuracy_score(y_test_claimants, y_pred_claimants) * 100))\n",
    "print(\"\\nF1 Score: {:.2f}\".format(f1_score(y_test_claimants, y_pred_claimants, average='macro') * 100))\n",
    "print(\"\\nConfusion Matrix:\\n\", confusion_matrix(y_test_claimants, y_pred_claimants))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../models/train_claimants_svm.pkl\", 'wb') as f:\n",
    "    pickle.dump(pipeline4, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import ADASYN\n",
    "\n",
    "ad = ADASYN()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe1 = Pipeline([('cv', cv),('sm', sm), ('nb', nb)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "         steps=[('cv',\n",
       "                 CountVectorizer(analyzer='word', binary=False,\n",
       "                                 decode_error='strict',\n",
       "                                 dtype=<class 'numpy.int64'>, encoding='utf-8',\n",
       "                                 input='content', lowercase=True, max_df=1.0,\n",
       "                                 max_features=None, min_df=1,\n",
       "                                 ngram_range=(1, 1), preprocessor=None,\n",
       "                                 stop_words=None, strip_accents=None,\n",
       "                                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                 tokenizer=None, vocabulary=None)),\n",
       "                ('sm',\n",
       "                 SMOTE(k_neighbors=5, kind='deprecated',\n",
       "                       m_neighbors='deprecated', n_jobs=1,\n",
       "                       out_step='deprecated', random_state=None, ratio=None,\n",
       "                       sampling_strategy='auto', svm_estimator='deprecated')),\n",
       "                ('nb',\n",
       "                 MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True))],\n",
       "         verbose=False)"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe1.fit(X_train_claimants, y_train_claimants)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
