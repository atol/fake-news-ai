{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import pickle\n",
    "import os\n",
    "\n",
    "METADATA_FILEPATH = '../dataset/metadata.json'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Source: https://towardsdatascience.com/machine-learning-word-embedding-sentiment-classification-using-keras-b83c28087456"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(METADATA_FILEPATH, 'r') as f:\n",
    "    claims = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(claims)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>claim</th>\n",
       "      <th>claimant</th>\n",
       "      <th>date</th>\n",
       "      <th>label</th>\n",
       "      <th>related_articles</th>\n",
       "      <th>id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>A line from George Orwell's novel 1984 predict...</td>\n",
       "      <td></td>\n",
       "      <td>2017-07-17</td>\n",
       "      <td>0</td>\n",
       "      <td>[122094, 122580, 130685, 134765]</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>Maine legislature candidate Leslie Gibson insu...</td>\n",
       "      <td></td>\n",
       "      <td>2018-03-17</td>\n",
       "      <td>2</td>\n",
       "      <td>[106868, 127320, 128060]</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>A 17-year-old girl named Alyssa Carson is bein...</td>\n",
       "      <td></td>\n",
       "      <td>2018-07-18</td>\n",
       "      <td>1</td>\n",
       "      <td>[132130, 132132, 149722]</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>In 1988 author Roald Dahl penned an open lette...</td>\n",
       "      <td></td>\n",
       "      <td>2019-02-04</td>\n",
       "      <td>2</td>\n",
       "      <td>[123254, 123418, 127464]</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>When it comes to fighting terrorism, \"Another ...</td>\n",
       "      <td>Hillary Clinton</td>\n",
       "      <td>2016-03-22</td>\n",
       "      <td>2</td>\n",
       "      <td>[41099, 89899, 72543, 82644, 95344, 88361]</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>Rhode Island is \"almost dead last\" among North...</td>\n",
       "      <td>Leonidas Raptakis</td>\n",
       "      <td>2014-02-11</td>\n",
       "      <td>2</td>\n",
       "      <td>[8284, 3768, 20091, 82368, 73148, 4493]</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>The poorest counties in the U.S. are in Appala...</td>\n",
       "      <td>Jim Webb</td>\n",
       "      <td>2014-11-19</td>\n",
       "      <td>1</td>\n",
       "      <td>[70709, 70708]</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>Koch Industries paid the legal fees of George ...</td>\n",
       "      <td></td>\n",
       "      <td>2013-07-18</td>\n",
       "      <td>0</td>\n",
       "      <td>[120591, 120592, 127866, 129483]</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>\"Minnesota, Michigan, Iowa already have 70 mph...</td>\n",
       "      <td>Robin Vos</td>\n",
       "      <td>2013-08-22</td>\n",
       "      <td>1</td>\n",
       "      <td>[69547, 80095, 7994, 81116, 77621]</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>\"FBI Uniform Crime Report for 2016 shows more ...</td>\n",
       "      <td>Nick Schroer</td>\n",
       "      <td>2017-10-17</td>\n",
       "      <td>1</td>\n",
       "      <td>[72012, 26005, 43481, 55671]</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               claim           claimant  \\\n",
       "0  A line from George Orwell's novel 1984 predict...                      \n",
       "1  Maine legislature candidate Leslie Gibson insu...                      \n",
       "2  A 17-year-old girl named Alyssa Carson is bein...                      \n",
       "3  In 1988 author Roald Dahl penned an open lette...                      \n",
       "4  When it comes to fighting terrorism, \"Another ...    Hillary Clinton   \n",
       "5  Rhode Island is \"almost dead last\" among North...  Leonidas Raptakis   \n",
       "6  The poorest counties in the U.S. are in Appala...           Jim Webb   \n",
       "7  Koch Industries paid the legal fees of George ...                      \n",
       "8  \"Minnesota, Michigan, Iowa already have 70 mph...          Robin Vos   \n",
       "9  \"FBI Uniform Crime Report for 2016 shows more ...       Nick Schroer   \n",
       "\n",
       "         date  label                            related_articles  id  \n",
       "0  2017-07-17      0            [122094, 122580, 130685, 134765]   0  \n",
       "1  2018-03-17      2                    [106868, 127320, 128060]   1  \n",
       "2  2018-07-18      1                    [132130, 132132, 149722]   4  \n",
       "3  2019-02-04      2                    [123254, 123418, 127464]   5  \n",
       "4  2016-03-22      2  [41099, 89899, 72543, 82644, 95344, 88361]   6  \n",
       "5  2014-02-11      2     [8284, 3768, 20091, 82368, 73148, 4493]   7  \n",
       "6  2014-11-19      1                              [70709, 70708]   8  \n",
       "7  2013-07-18      0            [120591, 120592, 127866, 129483]   9  \n",
       "8  2013-08-22      1          [69547, 80095, 7994, 81116, 77621]  11  \n",
       "9  2017-10-17      1                [72012, 26005, 43481, 55671]  12  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15555"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train = df.loc[:12444, 'claim'].values.tolist()\n",
    "# y_train = df.loc[:12444, 'label'].values.tolist()\n",
    "# X_test = df.loc[12445:, 'claim'].values.tolist()\n",
    "# y_test = df.loc[12445:, 'label'].values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenizer_obj = Tokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# total_claims = np.concatenate((X_train, X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenizer_obj.fit_on_texts(total_claims)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# max_length = max([len(s.split()) for s in total_claims])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# max_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vocab_size = len(tokenizer_obj.word_index) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "import gensim\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "EMBEDDING_DIM = 100\n",
    "max_length = 150"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "claim_lines = list()\n",
    "lines = df['claim'].values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "for line in lines:\n",
    "    tokens = word_tokenize(line)\n",
    "    tokens = [w.lower() for w in tokens]\n",
    "    table = str.maketrans('', '', string.punctuation)\n",
    "    stripped = [w.translate(table) for w in tokens]\n",
    "    words = [word for word in stripped if word.isalpha()]\n",
    "    claim_lines.append(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15555"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(claim_lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train word2vec model\n",
    "model = gensim.models.Word2Vec(sentences=claim_lines, size=EMBEDDING_DIM, window=5, workers=4, min_count=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabularly size: 24086\n"
     ]
    }
   ],
   "source": [
    "# Vocab size\n",
    "words = list(model.wv.vocab)\n",
    "print(\"Vocabularly size: %d\" % len (words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('insulting', 0.8759750723838806),\n",
       " ('obama', 0.8573005795478821),\n",
       " ('notation', 0.8439675569534302),\n",
       " ('analytics', 0.8419066071510315),\n",
       " ('vice', 0.8057959079742432),\n",
       " ('assad', 0.7952134609222412),\n",
       " ('emmy', 0.7938637733459473),\n",
       " ('limousine', 0.7933038473129272),\n",
       " ('boycotting', 0.782264232635498),\n",
       " ('zuma', 0.7754181027412415)]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.most_similar('trump')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('barack', 0.9336286187171936),\n",
       " ('donald', 0.928165078163147),\n",
       " ('administration', 0.9179438352584839),\n",
       " ('michelle', 0.9096911549568176),\n",
       " ('melania', 0.8953635096549988),\n",
       " ('wiretapping', 0.8885258436203003),\n",
       " ('presidency', 0.8832364082336426),\n",
       " ('ivanka', 0.8801191449165344),\n",
       " ('cleared', 0.8706070184707642),\n",
       " ('blurted', 0.8679291605949402)]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.most_similar('president')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('midterm', 0.9479297399520874),\n",
       " ('recount', 0.9437222480773926),\n",
       " ('2012', 0.9411381483078003),\n",
       " ('cycle', 0.9408465623855591),\n",
       " ('2008', 0.9364829063415527),\n",
       " ('rotc', 0.9358514547348022),\n",
       " ('collapsed', 0.9320218563079834),\n",
       " ('monday', 0.9263314604759216),\n",
       " ('norfolk', 0.9243888854980469),\n",
       " ('canals', 0.9207191467285156)]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.most_similar('election')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model\n",
    "filename = 'claims_embedding_word2vec_clean.txt'\n",
    "model.wv.save_word2vec_format(filename, binary=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_index = {}\n",
    "with open(\"claims_embedding_word2vec_clean.txt\", 'r') as f:\n",
    "    for line in f:\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        coefs = np.asarray(values[1:])\n",
    "        embeddings_index[word] = coefs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vectorize the text samples into a 2D integer tensor\n",
    "tokenizer_obj = Tokenizer()\n",
    "tokenizer_obj.fit_on_texts(claim_lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequences = tokenizer_obj.texts_to_sequences(claim_lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 24086 unique tokens.\n"
     ]
    }
   ],
   "source": [
    "# Pad sequences\n",
    "word_index = tokenizer_obj.word_index\n",
    "print(\"Found %s unique tokens.\" % len(word_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "claim_pad = pad_sequences(sequences, maxlen=max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "label = df['label'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of claim tensor: (15555, 150)\n"
     ]
    }
   ],
   "source": [
    "print(\"Shape of claim tensor:\", claim_pad.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of label tensor: (15555,)\n"
     ]
    }
   ],
   "source": [
    "print(\"Shape of label tensor:\", label.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_words = len(word_index) + 1\n",
    "embedding_matrix = np.zeros((num_words, EMBEDDING_DIM))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "for word, i in word_index.items():\n",
    "    if i > num_words:\n",
    "        continue\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        # Words not found in embedding index will be all-zeros\n",
    "        embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24087\n"
     ]
    }
   ],
   "source": [
    "print(num_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Embedding, LSTM, GRU\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.initializers import Constant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define model\n",
    "model = Sequential()\n",
    "embedding_layer = Embedding(num_words,\n",
    "                           EMBEDDING_DIM,\n",
    "                           embeddings_initializer=Constant(embedding_matrix),\n",
    "                           input_length=max_length,\n",
    "                           trainable=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(embedding_layer)\n",
    "model.add(GRU(units=32, dropout=0.1, recurrent_dropout=0.1))\n",
    "model.add(Dense(1, activation='relu'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_2 (Embedding)      (None, 150, 100)          2408700   \n",
      "_________________________________________________________________\n",
      "gru_2 (GRU)                  (None, 32)                12768     \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1)                 33        \n",
      "=================================================================\n",
      "Total params: 2,421,501\n",
      "Trainable params: 12,801\n",
      "Non-trainable params: 2,408,700\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into a training set and a validation set\n",
    "VALIDATION_SPLIT = 0.2\n",
    "\n",
    "indices = np.arange(claim_pad.shape[0])\n",
    "np.random.shuffle(indices)\n",
    "claim_pad = claim_pad[indices]\n",
    "label = label[indices]\n",
    "num_validation_samples = int(VALIDATION_SPLIT * claim_pad.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_pad = claim_pad[:-num_validation_samples]\n",
    "y_train = label[:-num_validation_samples]\n",
    "X_test_pad = claim_pad[-num_validation_samples:]\n",
    "y_test = label[-num_validation_samples:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X_train_pad tensor: (12444, 150)\n",
      "Shape of y_train tensor: (12444,)\n",
      "Shape of X_test_pad tensor: (3111, 150)\n",
      "Shape of y_test tensor: (3111,)\n"
     ]
    }
   ],
   "source": [
    "print(\"Shape of X_train_pad tensor:\", X_train_pad.shape)\n",
    "print(\"Shape of y_train tensor:\", y_train.shape)\n",
    "\n",
    "print(\"Shape of X_test_pad tensor:\", X_test_pad.shape)\n",
    "print(\"Shape of y_test tensor:\", y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training...\n",
      "Train on 12444 samples, validate on 3111 samples\n",
      "Epoch 1/25\n",
      " - 43s - loss: 2.1674 - accuracy: 0.5029 - val_loss: 1.0397 - val_accuracy: 0.4738\n",
      "Epoch 2/25\n",
      " - 39s - loss: 1.5069 - accuracy: 0.4782 - val_loss: 1.0231 - val_accuracy: 0.4796\n",
      "Epoch 3/25\n",
      " - 40s - loss: 0.9891 - accuracy: 0.4819 - val_loss: 0.6510 - val_accuracy: 0.4941\n",
      "Epoch 4/25\n",
      " - 40s - loss: 0.7797 - accuracy: 0.4781 - val_loss: 0.6208 - val_accuracy: 0.4632\n",
      "Epoch 5/25\n",
      " - 38s - loss: 0.9777 - accuracy: 0.5033 - val_loss: 1.0217 - val_accuracy: 0.4754\n",
      "Epoch 6/25\n",
      " - 38s - loss: 1.3523 - accuracy: 0.4928 - val_loss: 2.4611 - val_accuracy: 0.4458\n",
      "Epoch 7/25\n",
      " - 38s - loss: 0.9181 - accuracy: 0.4823 - val_loss: 0.7748 - val_accuracy: 0.4616\n",
      "Epoch 8/25\n",
      " - 37s - loss: 0.8325 - accuracy: 0.4743 - val_loss: 0.7741 - val_accuracy: 0.4648\n",
      "Epoch 9/25\n",
      " - 38s - loss: 0.7597 - accuracy: 0.4869 - val_loss: 0.6383 - val_accuracy: 0.4834\n",
      "Epoch 10/25\n",
      " - 37s - loss: 0.6509 - accuracy: 0.4803 - val_loss: 0.6193 - val_accuracy: 0.4802\n",
      "Epoch 11/25\n",
      " - 38s - loss: 3.0300 - accuracy: 0.3956 - val_loss: 2.8475 - val_accuracy: 0.3803\n",
      "Epoch 12/25\n",
      " - 37s - loss: 3.1232 - accuracy: 0.3871 - val_loss: 5.0999 - val_accuracy: 0.1189\n",
      "Epoch 13/25\n",
      " - 36s - loss: 5.4857 - accuracy: 0.1270 - val_loss: 5.0351 - val_accuracy: 0.1373\n",
      "Epoch 14/25\n",
      " - 36s - loss: 5.0180 - accuracy: 0.2130 - val_loss: 0.8809 - val_accuracy: 0.4963\n",
      "Epoch 15/25\n",
      " - 37s - loss: 1.2259 - accuracy: 0.4712 - val_loss: 0.7279 - val_accuracy: 0.4542\n",
      "Epoch 16/25\n",
      " - 41s - loss: 0.7356 - accuracy: 0.4757 - val_loss: 0.7039 - val_accuracy: 0.4796\n",
      "Epoch 17/25\n",
      " - 37s - loss: 0.7722 - accuracy: 0.4802 - val_loss: 0.6397 - val_accuracy: 0.4757\n",
      "Epoch 18/25\n",
      " - 36s - loss: 0.6626 - accuracy: 0.4849 - val_loss: 0.6356 - val_accuracy: 0.4626\n",
      "Epoch 19/25\n",
      " - 39s - loss: 0.6493 - accuracy: 0.4789 - val_loss: 0.6414 - val_accuracy: 0.4513\n",
      "Epoch 20/25\n",
      " - 39s - loss: 0.6568 - accuracy: 0.4997 - val_loss: 0.6099 - val_accuracy: 0.4793\n",
      "Epoch 21/25\n",
      " - 39s - loss: 0.6353 - accuracy: 0.4965 - val_loss: 0.6309 - val_accuracy: 0.4838\n",
      "Epoch 22/25\n",
      " - 39s - loss: 0.6559 - accuracy: 0.4636 - val_loss: 0.6139 - val_accuracy: 0.4667\n",
      "Epoch 23/25\n",
      " - 43s - loss: 0.6254 - accuracy: 0.4797 - val_loss: 0.6070 - val_accuracy: 0.4658\n",
      "Epoch 24/25\n",
      " - 45s - loss: 0.6181 - accuracy: 0.4819 - val_loss: 0.6068 - val_accuracy: 0.4725\n",
      "Epoch 25/25\n",
      " - 43s - loss: 0.6137 - accuracy: 0.4946 - val_loss: 0.6059 - val_accuracy: 0.4780\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x7f5bca138eb8>"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train_pad, y_train, batch_size=50, epochs=25, validation_data=(X_test_pad, y_test), verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
