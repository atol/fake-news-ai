{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>claim</th>\n",
       "      <th>claimant</th>\n",
       "      <th>date</th>\n",
       "      <th>label</th>\n",
       "      <th>related_articles</th>\n",
       "      <th>id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>9389</td>\n",
       "      <td>While arguing over President Reagan’s 1981 tax...</td>\n",
       "      <td>Sarah Sanders</td>\n",
       "      <td>2017-10-31</td>\n",
       "      <td>1</td>\n",
       "      <td>[34218, 55700, 18736, 39031, 34219, 34220]</td>\n",
       "      <td>10354</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1861</td>\n",
       "      <td>Recently Rick Scott \"closed 30 women’s health ...</td>\n",
       "      <td>Lois Frankel</td>\n",
       "      <td>2014-09-12</td>\n",
       "      <td>0</td>\n",
       "      <td>[73190, 76997, 38841, 77415, 77303, 9280, 8332...</td>\n",
       "      <td>2053</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11035</td>\n",
       "      <td>Says Target installed urinals in a women’s bat...</td>\n",
       "      <td>Facebook posts</td>\n",
       "      <td>2016-04-22</td>\n",
       "      <td>0</td>\n",
       "      <td>[9619, 22197]</td>\n",
       "      <td>12160</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12221</td>\n",
       "      <td>Says \"combined doses of vaccines\" have never b...</td>\n",
       "      <td>Facebook posts</td>\n",
       "      <td>2019-04-15</td>\n",
       "      <td>0</td>\n",
       "      <td>[57163, 31528, 40908, 31536, 68904, 44601]</td>\n",
       "      <td>13458</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11354</td>\n",
       "      <td>:   The AMBER Alert system has been discontinu...</td>\n",
       "      <td></td>\n",
       "      <td>2013-10-13</td>\n",
       "      <td>0</td>\n",
       "      <td>[103978, 121475, 121849]</td>\n",
       "      <td>12504</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2910</td>\n",
       "      <td>Health insurance costs for Floridians are up 3...</td>\n",
       "      <td>Republican Party of Florida</td>\n",
       "      <td>2014-09-23</td>\n",
       "      <td>1</td>\n",
       "      <td>[9581, 89571, 7836, 7945, 7949, 77360, 83491, ...</td>\n",
       "      <td>3208</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6096</td>\n",
       "      <td>A photograph captures Harriet Tubman as a \"Gun...</td>\n",
       "      <td></td>\n",
       "      <td>2019-03-25</td>\n",
       "      <td>0</td>\n",
       "      <td>[125108, 125968, 126005]</td>\n",
       "      <td>6701</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10446</td>\n",
       "      <td>ISIS leader Abu Bakr al Baghdadi was \"released...</td>\n",
       "      <td>Jeanine  Pirro</td>\n",
       "      <td>2014-06-14</td>\n",
       "      <td>0</td>\n",
       "      <td>[80115, 93998, 5968, 175, 91475, 8710, 89881, ...</td>\n",
       "      <td>11514</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5414</td>\n",
       "      <td>\"The board of a nonprofit organization on whic...</td>\n",
       "      <td>Tennessee Republican Party</td>\n",
       "      <td>2008-02-25</td>\n",
       "      <td>1</td>\n",
       "      <td>[96453, 71123, 61, 69968, 96477]</td>\n",
       "      <td>5966</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6657</td>\n",
       "      <td>Japan announced plans to dump 920,000 tons of ...</td>\n",
       "      <td></td>\n",
       "      <td>2017-08-22</td>\n",
       "      <td>1</td>\n",
       "      <td>[120293, 132262]</td>\n",
       "      <td>7328</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>12444 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   claim  \\\n",
       "9389   While arguing over President Reagan’s 1981 tax...   \n",
       "1861   Recently Rick Scott \"closed 30 women’s health ...   \n",
       "11035  Says Target installed urinals in a women’s bat...   \n",
       "12221  Says \"combined doses of vaccines\" have never b...   \n",
       "11354  :   The AMBER Alert system has been discontinu...   \n",
       "...                                                  ...   \n",
       "2910   Health insurance costs for Floridians are up 3...   \n",
       "6096   A photograph captures Harriet Tubman as a \"Gun...   \n",
       "10446  ISIS leader Abu Bakr al Baghdadi was \"released...   \n",
       "5414   \"The board of a nonprofit organization on whic...   \n",
       "6657   Japan announced plans to dump 920,000 tons of ...   \n",
       "\n",
       "                          claimant        date  label  \\\n",
       "9389                 Sarah Sanders  2017-10-31      1   \n",
       "1861                  Lois Frankel  2014-09-12      0   \n",
       "11035               Facebook posts  2016-04-22      0   \n",
       "12221               Facebook posts  2019-04-15      0   \n",
       "11354                               2013-10-13      0   \n",
       "...                            ...         ...    ...   \n",
       "2910   Republican Party of Florida  2014-09-23      1   \n",
       "6096                                2019-03-25      0   \n",
       "10446               Jeanine  Pirro  2014-06-14      0   \n",
       "5414    Tennessee Republican Party  2008-02-25      1   \n",
       "6657                                2017-08-22      1   \n",
       "\n",
       "                                        related_articles     id  \n",
       "9389          [34218, 55700, 18736, 39031, 34219, 34220]  10354  \n",
       "1861   [73190, 76997, 38841, 77415, 77303, 9280, 8332...   2053  \n",
       "11035                                      [9619, 22197]  12160  \n",
       "12221         [57163, 31528, 40908, 31536, 68904, 44601]  13458  \n",
       "11354                           [103978, 121475, 121849]  12504  \n",
       "...                                                  ...    ...  \n",
       "2910   [9581, 89571, 7836, 7945, 7949, 77360, 83491, ...   3208  \n",
       "6096                            [125108, 125968, 126005]   6701  \n",
       "10446  [80115, 93998, 5968, 175, 91475, 8710, 89881, ...  11514  \n",
       "5414                    [96453, 71123, 61, 69968, 96477]   5966  \n",
       "6657                                    [120293, 132262]   7328  \n",
       "\n",
       "[12444 rows x 6 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_set = pd.read_pickle('train_set.pkl')\n",
    "train_set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_set = pd.read_pickle('test_set.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import sklearn\n",
    "from sklearn.metrics import accuracy_score, f1_score, confusion_matrix\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.pipeline import make_pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Source: https://www.learndatasci.com/tutorials/predicting-reddit-news-sentiment-naive-bayes-text-classifiers/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = train_set.claim\n",
    "y_train = train_set.label\n",
    "X_test = test_set.claim\n",
    "y_test = test_set.label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create pipeline for the model\n",
    "model = make_pipeline(CountVectorizer(), MultinomialNB())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "         steps=[('countvectorizer',\n",
       "                 CountVectorizer(analyzer='word', binary=False,\n",
       "                                 decode_error='strict',\n",
       "                                 dtype=<class 'numpy.int64'>, encoding='utf-8',\n",
       "                                 input='content', lowercase=True, max_df=1.0,\n",
       "                                 max_features=None, min_df=1,\n",
       "                                 ngram_range=(1, 1), preprocessor=None,\n",
       "                                 stop_words=None, strip_accents=None,\n",
       "                                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                 tokenizer=None, vocabulary=None)),\n",
       "                ('multinomialnb',\n",
       "                 MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True))],\n",
       "         verbose=False)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train the model\n",
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use model to make predictions\n",
    "y_pred = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6039858566377371"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check accuracy of model\n",
    "accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 60.40%\n",
      "\n",
      "F1 Score: 44.18\n",
      "\n",
      "Confusion Matrix:\n",
      " [[923 522   9]\n",
      " [377 947  10]\n",
      " [121 193   9]]\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy: {:.2f}%\".format(accuracy_score(y_test, y_pred) * 100))\n",
    "print(\"\\nF1 Score: {:.2f}\".format(f1_score(y_test, y_pred, average='macro') * 100))\n",
    "print(\"\\nConfusion Matrix:\\n\", confusion_matrix(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    5954\n",
      "1    5117\n",
      "2    1373\n",
      "Name: label, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "counts = train_set.label.value_counts()\n",
    "print(counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Predicting only 0 = 47.85% accuracy\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nPredicting only 0 = {:.2f}% accuracy\".format(counts[0] / sum(counts) * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Predicting only 1 = 41.12% accuracy\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nPredicting only 1 = {:.2f}% accuracy\".format(counts[1] / sum(counts) * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Predicting only 2 = 11.03% accuracy\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nPredicting only 2 = {:.2f}% accuracy\".format(counts[2] / sum(counts) * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "vect = CountVectorizer()\n",
    "\n",
    "X_train_vect = vect.fit_transform(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, '00'),\n",
       " (0, '000'),\n",
       " (0, '0000188'),\n",
       " (0, '000mw'),\n",
       " (0, '001'),\n",
       " (0, '004'),\n",
       " (0, '008'),\n",
       " (0, '01'),\n",
       " (0, '014'),\n",
       " (0, '018'),\n",
       " (0, '01c1537833bf99'),\n",
       " (0, '02'),\n",
       " (0, '029'),\n",
       " (0, '03'),\n",
       " (0, '033'),\n",
       " (0, '04'),\n",
       " (0, '042'),\n",
       " (0, '05'),\n",
       " (0, '050'),\n",
       " (0, '053'),\n",
       " (0, '054th'),\n",
       " (0, '06'),\n",
       " (0, '0607'),\n",
       " (0, '06072016_outstanding'),\n",
       " (0, '063'),\n",
       " (0, '07'),\n",
       " (0, '08'),\n",
       " (0, '084'),\n",
       " (0, '09'),\n",
       " (0, '0900'),\n",
       " (0, '095'),\n",
       " (0, '0cx1cqs9x8'),\n",
       " (0, '0hour'),\n",
       " (0, '0whllsofrn'),\n",
       " (0, '10'),\n",
       " (0, '100'),\n",
       " (0, '1000'),\n",
       " (0, '100011345193387'),\n",
       " (0, '10010'),\n",
       " (0, '1004'),\n",
       " (0, '100k'),\n",
       " (0, '100s'),\n",
       " (0, '100th'),\n",
       " (0, '101'),\n",
       " (0, '101st'),\n",
       " (0, '102'),\n",
       " (0, '10205607353878502'),\n",
       " (0, '1024'),\n",
       " (0, '103'),\n",
       " (0, '104'),\n",
       " (0, '1040'),\n",
       " (0, '105'),\n",
       " (0, '106'),\n",
       " (0, '107'),\n",
       " (0, '1070'),\n",
       " (0, '1073741828'),\n",
       " (0, '108'),\n",
       " (0, '108th'),\n",
       " (0, '109'),\n",
       " (0, '1092990525'),\n",
       " (0, '10990'),\n",
       " (0, '10995'),\n",
       " (0, '10997'),\n",
       " (0, '10998'),\n",
       " (0, '109th'),\n",
       " (0, '10am'),\n",
       " (0, '10s'),\n",
       " (0, '10sad'),\n",
       " (0, '10th'),\n",
       " (0, '11'),\n",
       " (0, '110'),\n",
       " (0, '11000'),\n",
       " (0, '11001'),\n",
       " (0, '11002'),\n",
       " (0, '11003'),\n",
       " (0, '11004'),\n",
       " (0, '11005'),\n",
       " (0, '11049'),\n",
       " (0, '11051'),\n",
       " (0, '111'),\n",
       " (0, '11105552'),\n",
       " (0, '111th'),\n",
       " (0, '112'),\n",
       " (0, '113'),\n",
       " (0, '11310'),\n",
       " (0, '1137883162600'),\n",
       " (0, '115'),\n",
       " (0, '116'),\n",
       " (0, '116th'),\n",
       " (0, '117'),\n",
       " (0, '119'),\n",
       " (0, '11921'),\n",
       " (0, '11k'),\n",
       " (0, '11m'),\n",
       " (0, '11pm'),\n",
       " (0, '11s'),\n",
       " (0, '11th'),\n",
       " (0, '12'),\n",
       " (0, '120'),\n",
       " (0, '12172'),\n",
       " (0, '122'),\n",
       " (0, '12206'),\n",
       " (0, '124'),\n",
       " (0, '125'),\n",
       " (0, '128'),\n",
       " (0, '129'),\n",
       " (0, '12pm'),\n",
       " (0, '12th'),\n",
       " (0, '13'),\n",
       " (0, '130'),\n",
       " (0, '131'),\n",
       " (0, '134'),\n",
       " (0, '135'),\n",
       " (0, '136'),\n",
       " (0, '1369'),\n",
       " (0, '137'),\n",
       " (0, '138'),\n",
       " (0, '13th'),\n",
       " (0, '14'),\n",
       " (0, '140'),\n",
       " (0, '141'),\n",
       " (0, '142'),\n",
       " (0, '143'),\n",
       " (0, '144'),\n",
       " (0, '144174049257316'),\n",
       " (0, '145'),\n",
       " (0, '146766985664689'),\n",
       " (0, '148'),\n",
       " (0, '1480949417'),\n",
       " (0, '14th'),\n",
       " (0, '15'),\n",
       " (0, '150'),\n",
       " (0, '151'),\n",
       " (0, '153'),\n",
       " (0, '154'),\n",
       " (0, '155'),\n",
       " (0, '1555'),\n",
       " (0, '156'),\n",
       " (0, '157'),\n",
       " (0, '158'),\n",
       " (0, '15s'),\n",
       " (0, '15th'),\n",
       " (0, '16'),\n",
       " (0, '160'),\n",
       " (0, '161'),\n",
       " (0, '1612'),\n",
       " (0, '1623371'),\n",
       " (0, '163'),\n",
       " (0, '164'),\n",
       " (0, '165'),\n",
       " (0, '168'),\n",
       " (0, '169'),\n",
       " (0, '16th'),\n",
       " (0, '17'),\n",
       " (0, '170'),\n",
       " (0, '1700s'),\n",
       " (0, '171'),\n",
       " (0, '172'),\n",
       " (0, '174'),\n",
       " (0, '174k'),\n",
       " (0, '177'),\n",
       " (0, '178'),\n",
       " (0, '1789'),\n",
       " (0, '179'),\n",
       " (0, '1790'),\n",
       " (0, '1791'),\n",
       " (0, '18'),\n",
       " (0, '180'),\n",
       " (0, '1800s'),\n",
       " (0, '180km'),\n",
       " (0, '181'),\n",
       " (0, '182'),\n",
       " (0, '1829'),\n",
       " (0, '183'),\n",
       " (0, '1835'),\n",
       " (0, '1836'),\n",
       " (0, '183rd'),\n",
       " (0, '185'),\n",
       " (0, '1850'),\n",
       " (0, '1858'),\n",
       " (0, '186'),\n",
       " (0, '1860'),\n",
       " (0, '1862'),\n",
       " (0, '1863'),\n",
       " (0, '187'),\n",
       " (0, '1875'),\n",
       " (0, '188'),\n",
       " (0, '1880'),\n",
       " (0, '1881'),\n",
       " (0, '1888'),\n",
       " (0, '1890'),\n",
       " (0, '1890s'),\n",
       " (0, '1898'),\n",
       " (0, '189th'),\n",
       " (0, '18s'),\n",
       " (0, '18th'),\n",
       " (0, '18wu9feave'),\n",
       " (0, '19'),\n",
       " (0, '190'),\n",
       " (0, '1900'),\n",
       " (0, '1900s'),\n",
       " (0, '1902'),\n",
       " (0, '1903'),\n",
       " (0, '1904'),\n",
       " (0, '1905'),\n",
       " (0, '1907'),\n",
       " (0, '1911'),\n",
       " (0, '1912'),\n",
       " (0, '1913'),\n",
       " (0, '1915'),\n",
       " (0, '1916'),\n",
       " (0, '1917'),\n",
       " (0, '1919'),\n",
       " (0, '1920s'),\n",
       " (0, '1924'),\n",
       " (0, '1926'),\n",
       " (0, '1927'),\n",
       " (0, '1928'),\n",
       " (0, '1929'),\n",
       " (0, '193'),\n",
       " (0, '1930'),\n",
       " (0, '1930s'),\n",
       " (0, '1933'),\n",
       " (0, '1935'),\n",
       " (0, '1936'),\n",
       " (0, '1937'),\n",
       " (0, '1938'),\n",
       " (0, '1939'),\n",
       " (0, '194'),\n",
       " (0, '1940'),\n",
       " (0, '1940s'),\n",
       " (0, '1941'),\n",
       " (0, '1942'),\n",
       " (0, '1944'),\n",
       " (0, '1945'),\n",
       " (0, '1947'),\n",
       " (0, '1948'),\n",
       " (0, '195'),\n",
       " (0, '1950'),\n",
       " (0, '1952'),\n",
       " (0, '1954'),\n",
       " (0, '1955'),\n",
       " (0, '1956'),\n",
       " (0, '1958'),\n",
       " (0, '1959'),\n",
       " (0, '196'),\n",
       " (0, '1960'),\n",
       " (0, '1960s'),\n",
       " (0, '1961'),\n",
       " (0, '1962'),\n",
       " (0, '1963'),\n",
       " (0, '1964'),\n",
       " (0, '1965'),\n",
       " (0, '1966'),\n",
       " (0, '1967'),\n",
       " (0, '1968'),\n",
       " (0, '1969'),\n",
       " (0, '197'),\n",
       " (0, '1970'),\n",
       " (0, '1970s'),\n",
       " (0, '1971'),\n",
       " (0, '1972'),\n",
       " (0, '1973'),\n",
       " (0, '1975'),\n",
       " (0, '1976'),\n",
       " (0, '1977'),\n",
       " (0, '1978'),\n",
       " (0, '1979'),\n",
       " (0, '1980'),\n",
       " (0, '1980s'),\n",
       " (0, '1981'),\n",
       " (0, '1982'),\n",
       " (0, '1983'),\n",
       " (0, '1984'),\n",
       " (0, '1985'),\n",
       " (0, '1986'),\n",
       " (0, '1987'),\n",
       " (0, '1988'),\n",
       " (0, '1989'),\n",
       " (0, '1990'),\n",
       " (0, '1990s'),\n",
       " (0, '1991'),\n",
       " (0, '1992'),\n",
       " (0, '1993'),\n",
       " (0, '1994'),\n",
       " (0, '1995'),\n",
       " (0, '1996'),\n",
       " (0, '1997'),\n",
       " (0, '1998'),\n",
       " (0, '1999'),\n",
       " (0, '19th'),\n",
       " (0, '1dittlinger'),\n",
       " (0, '1fsvtqymmd'),\n",
       " (0, '1m'),\n",
       " (0, '1rachacsecurity'),\n",
       " (0, '1st'),\n",
       " (0, '1utlwbtc9s'),\n",
       " (0, '20'),\n",
       " (0, '200'),\n",
       " (0, '2000'),\n",
       " (0, '2000s'),\n",
       " (0, '2001'),\n",
       " (0, '2002'),\n",
       " (0, '2003'),\n",
       " (0, '2004'),\n",
       " (0, '2005'),\n",
       " (0, '2006'),\n",
       " (0, '2007'),\n",
       " (0, '2008'),\n",
       " (0, '2009'),\n",
       " (0, '200lb'),\n",
       " (0, '200名以上のhivキャリアを雇用し缶製造'),\n",
       " (0, '2010'),\n",
       " (0, '2011'),\n",
       " (0, '2012'),\n",
       " (0, '2013'),\n",
       " (0, '2014'),\n",
       " (0, '2015'),\n",
       " (0, '2016'),\n",
       " (0, '2016emory'),\n",
       " (0, '2016i'),\n",
       " (0, '2016in'),\n",
       " (0, '2017'),\n",
       " (0, '2018'),\n",
       " (0, '2019'),\n",
       " (0, '202'),\n",
       " (0, '2020'),\n",
       " (0, '2021'),\n",
       " (0, '2021722'),\n",
       " (0, '2022'),\n",
       " (0, '2025'),\n",
       " (0, '2026'),\n",
       " (0, '2030'),\n",
       " (0, '2035'),\n",
       " (0, '2038'),\n",
       " (0, '204'),\n",
       " (0, '2040'),\n",
       " (0, '2050'),\n",
       " (0, '207'),\n",
       " (0, '2070'),\n",
       " (0, '209'),\n",
       " (0, '20committee'),\n",
       " (0, '20k'),\n",
       " (0, '20minutenews'),\n",
       " (0, '20th'),\n",
       " (0, '20yrs'),\n",
       " (0, '21'),\n",
       " (0, '210'),\n",
       " (0, '2100'),\n",
       " (0, '210406355967418'),\n",
       " (0, '213'),\n",
       " (0, '214'),\n",
       " (0, '215'),\n",
       " (0, '218'),\n",
       " (0, '219'),\n",
       " (0, '21st'),\n",
       " (0, '21t'),\n",
       " (0, '21wjfyc4rf'),\n",
       " (0, '22'),\n",
       " (0, '220'),\n",
       " (0, '2207520000'),\n",
       " (0, '221'),\n",
       " (0, '223'),\n",
       " (0, '2231'),\n",
       " (0, '224'),\n",
       " (0, '225'),\n",
       " (0, '227'),\n",
       " (0, '228'),\n",
       " (0, '22nd'),\n",
       " (0, '23'),\n",
       " (0, '230'),\n",
       " (0, '233'),\n",
       " (0, '235'),\n",
       " (0, '236'),\n",
       " (0, '237'),\n",
       " (0, '238'),\n",
       " (0, '2397164'),\n",
       " (0, '23rd'),\n",
       " (0, '24'),\n",
       " (0, '240'),\n",
       " (0, '242'),\n",
       " (0, '244'),\n",
       " (0, '245'),\n",
       " (0, '247'),\n",
       " (0, '2478714'),\n",
       " (0, '248'),\n",
       " (0, '25'),\n",
       " (0, '250'),\n",
       " (0, '2500'),\n",
       " (0, '2521'),\n",
       " (0, '256'),\n",
       " (0, '25k'),\n",
       " (0, '25th'),\n",
       " (0, '25x'),\n",
       " (0, '26'),\n",
       " (0, '260'),\n",
       " (0, '267'),\n",
       " (0, '26th'),\n",
       " (0, '27'),\n",
       " (0, '270'),\n",
       " (0, '271'),\n",
       " (0, '272'),\n",
       " (0, '273rd'),\n",
       " (0, '275'),\n",
       " (0, '278'),\n",
       " (0, '27th'),\n",
       " (0, '28'),\n",
       " (0, '280'),\n",
       " (0, '282'),\n",
       " (0, '2847'),\n",
       " (0, '285'),\n",
       " (0, '285000'),\n",
       " (0, '287'),\n",
       " (0, '28th'),\n",
       " (0, '29'),\n",
       " (0, '293'),\n",
       " (0, '294'),\n",
       " (0, '2943'),\n",
       " (0, '295x'),\n",
       " (0, '296'),\n",
       " (0, '297'),\n",
       " (0, '299'),\n",
       " (0, '2a'),\n",
       " (0, '2am'),\n",
       " (0, '2godbglory1'),\n",
       " (0, '2kztoxy'),\n",
       " (0, '2nd'),\n",
       " (1, '30'),\n",
       " (0, '300'),\n",
       " (0, '306'),\n",
       " (0, '308'),\n",
       " (0, '30s'),\n",
       " (0, '30th'),\n",
       " (0, '31'),\n",
       " (0, '313'),\n",
       " (0, '317'),\n",
       " (0, '31st'),\n",
       " (0, '32'),\n",
       " (0, '320'),\n",
       " (0, '3200'),\n",
       " (0, '321'),\n",
       " (0, '324'),\n",
       " (0, '325'),\n",
       " (0, '32t'),\n",
       " (0, '33'),\n",
       " (0, '330'),\n",
       " (0, '331'),\n",
       " (0, '333'),\n",
       " (0, '34'),\n",
       " (0, '340'),\n",
       " (0, '343'),\n",
       " (0, '345'),\n",
       " (0, '347'),\n",
       " (0, '348'),\n",
       " (0, '34th'),\n",
       " (0, '35'),\n",
       " (0, '350'),\n",
       " (0, '353'),\n",
       " (0, '355'),\n",
       " (0, '358'),\n",
       " (0, '35s'),\n",
       " (0, '36'),\n",
       " (0, '360'),\n",
       " (0, '362'),\n",
       " (0, '362020267519477'),\n",
       " (0, '365'),\n",
       " (0, '37'),\n",
       " (0, '370'),\n",
       " (0, '371feca4'),\n",
       " (0, '37th'),\n",
       " (0, '38'),\n",
       " (0, '380'),\n",
       " (0, '380th'),\n",
       " (0, '381'),\n",
       " (0, '389'),\n",
       " (0, '38th'),\n",
       " (0, '39'),\n",
       " (0, '399'),\n",
       " (0, '3c'),\n",
       " (0, '3d'),\n",
       " (0, '3dchpfgg6v'),\n",
       " (0, '3dem9wqi74'),\n",
       " (0, '3e'),\n",
       " (0, '3gva1160k'),\n",
       " (0, '3m'),\n",
       " (0, '3p4sjruiem'),\n",
       " (0, '3rd'),\n",
       " (0, '3е'),\n",
       " (0, '40'),\n",
       " (0, '400'),\n",
       " (0, '404'),\n",
       " (0, '40s'),\n",
       " (0, '41'),\n",
       " (0, '410'),\n",
       " (0, '412'),\n",
       " (0, '413'),\n",
       " (0, '41st'),\n",
       " (0, '42'),\n",
       " (0, '420'),\n",
       " (0, '425'),\n",
       " (0, '427'),\n",
       " (0, '428'),\n",
       " (0, '43'),\n",
       " (0, '430'),\n",
       " (0, '431'),\n",
       " (0, '438'),\n",
       " (0, '43rd'),\n",
       " (0, '44'),\n",
       " (0, '440'),\n",
       " (0, '4425'),\n",
       " (0, '443'),\n",
       " (0, '444'),\n",
       " (0, '445'),\n",
       " (0, '44s'),\n",
       " (0, '44zj6x1m3r'),\n",
       " (0, '45'),\n",
       " (0, '450'),\n",
       " (0, '45br8u'),\n",
       " (0, '45th'),\n",
       " (0, '46'),\n",
       " (0, '466'),\n",
       " (0, '47'),\n",
       " (0, '47s'),\n",
       " (0, '47th'),\n",
       " (0, '48'),\n",
       " (0, '481'),\n",
       " (0, '485'),\n",
       " (0, '488'),\n",
       " (0, '48th'),\n",
       " (0, '49'),\n",
       " (0, '4919'),\n",
       " (0, '495'),\n",
       " (0, '497'),\n",
       " (0, '49ers'),\n",
       " (0, '49th'),\n",
       " (0, '4blynph5qo'),\n",
       " (0, '4mhxiiwsjb'),\n",
       " (0, '4th'),\n",
       " (0, '4umrz4jl0e'),\n",
       " (0, '50'),\n",
       " (0, '500'),\n",
       " (0, '5000'),\n",
       " (0, '500k'),\n",
       " (0, '501'),\n",
       " (0, '504'),\n",
       " (0, '505'),\n",
       " (0, '50k'),\n",
       " (0, '50th'),\n",
       " (0, '51'),\n",
       " (0, '512'),\n",
       " (0, '513'),\n",
       " (0, '51yrs'),\n",
       " (0, '52'),\n",
       " (0, '522'),\n",
       " (0, '52616'),\n",
       " (0, '53'),\n",
       " (0, '530'),\n",
       " (0, '535'),\n",
       " (0, '538'),\n",
       " (0, '54'),\n",
       " (0, '540'),\n",
       " (0, '547'),\n",
       " (0, '54lnxtmgc5'),\n",
       " (0, '55'),\n",
       " (0, '550'),\n",
       " (0, '56'),\n",
       " (0, '562'),\n",
       " (0, '566'),\n",
       " (0, '569'),\n",
       " (0, '57'),\n",
       " (0, '570'),\n",
       " (0, '572'),\n",
       " (0, '575'),\n",
       " (0, '58'),\n",
       " (0, '582'),\n",
       " (0, '585'),\n",
       " (0, '58th'),\n",
       " (0, '59'),\n",
       " (0, '5am'),\n",
       " (0, '5da0kp'),\n",
       " (0, '5dw5zu'),\n",
       " (0, '5g'),\n",
       " (0, '5lqc2llxkj'),\n",
       " (0, '5m'),\n",
       " (0, '5o1delleol'),\n",
       " (0, '5s'),\n",
       " (0, '5th'),\n",
       " (0, '60'),\n",
       " (0, '600'),\n",
       " (0, '600th'),\n",
       " (0, '608'),\n",
       " (0, '60s'),\n",
       " (0, '61'),\n",
       " (0, '611'),\n",
       " (0, '62'),\n",
       " (0, '620'),\n",
       " (0, '625'),\n",
       " (0, '63'),\n",
       " (0, '630'),\n",
       " (0, '634'),\n",
       " (0, '64'),\n",
       " (0, '643'),\n",
       " (0, '65'),\n",
       " (0, '650'),\n",
       " (0, '652'),\n",
       " (0, '657'),\n",
       " (0, '66'),\n",
       " (0, '6627'),\n",
       " (0, '666'),\n",
       " (0, '67'),\n",
       " (0, '671'),\n",
       " (0, '675'),\n",
       " (0, '68'),\n",
       " (0, '680'),\n",
       " (0, '685'),\n",
       " (0, '69'),\n",
       " (0, '694'),\n",
       " (0, '697'),\n",
       " (0, '69th'),\n",
       " (0, '6am'),\n",
       " (0, '6cpcmyndb9'),\n",
       " (0, '6e7fknaomz'),\n",
       " (0, '6grade56'),\n",
       " (0, '6m'),\n",
       " (0, '6qskp9pvls'),\n",
       " (0, '6th'),\n",
       " (0, '6zmbanbvyl'),\n",
       " (0, '70'),\n",
       " (0, '700'),\n",
       " (0, '705'),\n",
       " (0, '707397251606380544'),\n",
       " (0, '70s'),\n",
       " (0, '71'),\n",
       " (0, '712'),\n",
       " (0, '716'),\n",
       " (0, '72'),\n",
       " (0, '720'),\n",
       " (0, '73'),\n",
       " (0, '730'),\n",
       " (0, '733'),\n",
       " (0, '736'),\n",
       " (0, '737'),\n",
       " (0, '737m'),\n",
       " (0, '74'),\n",
       " (0, '740'),\n",
       " (0, '740641362979098624'),\n",
       " (0, '741741'),\n",
       " (0, '747'),\n",
       " (0, '74bbb4c47e8a'),\n",
       " (0, '74o8qmkw3n'),\n",
       " (0, '75'),\n",
       " (0, '750'),\n",
       " (0, '757'),\n",
       " (0, '76'),\n",
       " (0, '761'),\n",
       " (0, '762'),\n",
       " (0, '765'),\n",
       " (0, '768'),\n",
       " (0, '77'),\n",
       " (0, '772'),\n",
       " (0, '773'),\n",
       " (0, '775'),\n",
       " (0, '78'),\n",
       " (0, '786'),\n",
       " (0, '78702'),\n",
       " (0, '79'),\n",
       " (0, '794'),\n",
       " (0, '7in10forroe'),\n",
       " (0, '7pm'),\n",
       " (0, '7th'),\n",
       " (0, '7zexeb8sw1'),\n",
       " (0, '80'),\n",
       " (0, '800'),\n",
       " (0, '8000'),\n",
       " (0, '80000'),\n",
       " (0, '803'),\n",
       " (0, '805'),\n",
       " (0, '80s'),\n",
       " (0, '81'),\n",
       " (0, '813'),\n",
       " (0, '82'),\n",
       " (0, '82nd'),\n",
       " (0, '82packfan'),\n",
       " (0, '83'),\n",
       " (0, '83361'),\n",
       " (0, '84'),\n",
       " (0, '845'),\n",
       " (0, '846'),\n",
       " (0, '85'),\n",
       " (0, '850'),\n",
       " (0, '855'),\n",
       " (0, '86'),\n",
       " (0, '860'),\n",
       " (0, '87'),\n",
       " (0, '871'),\n",
       " (0, '874'),\n",
       " (0, '88'),\n",
       " (0, '882'),\n",
       " (0, '88th'),\n",
       " (0, '89'),\n",
       " (0, '890'),\n",
       " (0, '898'),\n",
       " (0, '89800'),\n",
       " (0, '899'),\n",
       " (0, '8b'),\n",
       " (0, '8feb2019'),\n",
       " (0, '8th'),\n",
       " (0, '8ys8lhgdan'),\n",
       " (0, '90'),\n",
       " (0, '900'),\n",
       " (0, '902'),\n",
       " (0, '90s'),\n",
       " (0, '90s0mbktj6'),\n",
       " (0, '91'),\n",
       " (0, '911'),\n",
       " (0, '916'),\n",
       " (0, '92'),\n",
       " (0, '920'),\n",
       " (0, '923'),\n",
       " (0, '93'),\n",
       " (0, '932'),\n",
       " (0, '94'),\n",
       " (0, '946'),\n",
       " (0, '95'),\n",
       " (0, '950'),\n",
       " (0, '96'),\n",
       " (0, '97'),\n",
       " (0, '971'),\n",
       " (0, '98'),\n",
       " (0, '980'),\n",
       " (0, '984m1mar1865tc'),\n",
       " (0, '99'),\n",
       " (0, '999'),\n",
       " (0, '9pm'),\n",
       " (0, '9rxm1hl6js'),\n",
       " (0, '9th'),\n",
       " (0, '9thcircuit'),\n",
       " (0,\n",
       "  '__________________________________________________________________________'),\n",
       " (0,\n",
       "  '_____________________________________________________________________________'),\n",
       " (0,\n",
       "  '____________________________________________________________________________________'),\n",
       " (0, '_ajcousins'),\n",
       " (0, 'a1kdrvy3'),\n",
       " (0, 'a3auntie'),\n",
       " (0, 'aa'),\n",
       " (0, 'aaa'),\n",
       " (0, 'aaron'),\n",
       " (0, 'aarp'),\n",
       " (0, 'ab'),\n",
       " (0, 'ababa'),\n",
       " (0, 'abandoned'),\n",
       " (0, 'abandoning'),\n",
       " (0, 'abbas'),\n",
       " (0, 'abbott'),\n",
       " (0, 'abc'),\n",
       " (0, 'abcnews'),\n",
       " (0, 'abdomen'),\n",
       " (0, 'abducted'),\n",
       " (0, 'abductions'),\n",
       " (0, 'abductors'),\n",
       " (0, 'abdul'),\n",
       " (0, 'abe'),\n",
       " (0, 'abedi'),\n",
       " (0, 'abedin'),\n",
       " (0, 'abele'),\n",
       " (0, 'abell'),\n",
       " (0, 'abide'),\n",
       " (0, 'abided'),\n",
       " (0, 'abides'),\n",
       " (0, 'abiding'),\n",
       " (0, 'abigail'),\n",
       " (0, 'abilene'),\n",
       " (0, 'abilities'),\n",
       " (0, 'ability'),\n",
       " (0, 'abject'),\n",
       " (0, 'abkhazia'),\n",
       " (0, 'able'),\n",
       " (0, 'abled'),\n",
       " (0, 'abm'),\n",
       " (0, 'aboard'),\n",
       " (0, 'abolish'),\n",
       " (0, 'abolished'),\n",
       " (0, 'abolishes'),\n",
       " (0, 'abolishing'),\n",
       " (0, 'abolition'),\n",
       " (0, 'aboriginal'),\n",
       " (0, 'abort'),\n",
       " (0, 'aborted'),\n",
       " (0, 'abortion'),\n",
       " (0, 'abortionist'),\n",
       " (0, 'abortions'),\n",
       " (0, 'about'),\n",
       " (0, 'above'),\n",
       " (0, 'abraham'),\n",
       " (0, 'abramoff'),\n",
       " (0, 'abramovich'),\n",
       " (0, 'abrams'),\n",
       " (0, 'abroad'),\n",
       " (0, 'abrupt'),\n",
       " (0, 'abruptly'),\n",
       " (0, 'absence'),\n",
       " (0, 'absent'),\n",
       " (0, 'absentee'),\n",
       " (0, 'absolute'),\n",
       " (0, 'absolutely'),\n",
       " (0, 'absorb'),\n",
       " (0, 'absorbed'),\n",
       " (0, 'absurd'),\n",
       " (0, 'absurdity'),\n",
       " (0, 'abt'),\n",
       " (0, 'abu'),\n",
       " (0, 'abubakar'),\n",
       " (0, 'abuja'),\n",
       " (0, 'abunimah'),\n",
       " (0, 'abuse'),\n",
       " (0, 'abused'),\n",
       " (0, 'abuser'),\n",
       " (0, 'abusers'),\n",
       " (0, 'abuses'),\n",
       " (0, 'abusing'),\n",
       " (0, 'abusive'),\n",
       " (0, 'aca'),\n",
       " (0, 'academic'),\n",
       " (0, 'academy'),\n",
       " (0, 'accelerated'),\n",
       " (0, 'accents'),\n",
       " (0, 'accept'),\n",
       " (0, 'acceptable'),\n",
       " (0, 'acceptance'),\n",
       " (0, 'accepted'),\n",
       " (0, 'accepting'),\n",
       " (0, 'accepts'),\n",
       " (0, 'access'),\n",
       " (0, 'accessed'),\n",
       " (0, 'accessible'),\n",
       " (0, 'accessing'),\n",
       " (0, 'accident'),\n",
       " (0, 'accidental'),\n",
       " (0, 'accidentally'),\n",
       " (0, 'accidents'),\n",
       " (0, 'acclaimed'),\n",
       " (0, 'accommodate'),\n",
       " (0, 'accommodating'),\n",
       " (0, 'accompanied'),\n",
       " (0, 'accompanying'),\n",
       " (0, 'accomplice'),\n",
       " (0, 'accomplish'),\n",
       " (0, 'accomplished'),\n",
       " (0, 'accomplishments'),\n",
       " (0, 'accord'),\n",
       " (0, 'accordance'),\n",
       " (0, 'according'),\n",
       " (0, 'accordingly'),\n",
       " (0, 'accordion'),\n",
       " (0, 'accosted'),\n",
       " (0, 'account'),\n",
       " (0, 'accountability'),\n",
       " (0, 'accountable'),\n",
       " (0, 'accounted'),\n",
       " (0, 'accounting'),\n",
       " (0, 'accounts'),\n",
       " (0, 'accreditation'),\n",
       " (0, 'accredited'),\n",
       " (0, 'accumulate'),\n",
       " (0, 'accumulated'),\n",
       " (0, 'accuracy'),\n",
       " (0, 'accurate'),\n",
       " (0, 'accurately'),\n",
       " (0, 'accusation'),\n",
       " (0, 'accusations'),\n",
       " (0, 'accuse'),\n",
       " (0, 'accused'),\n",
       " (0, 'accuser'),\n",
       " (0, 'accuses'),\n",
       " (0, 'accusing'),\n",
       " (0, 'accustomed'),\n",
       " (0, 'ace'),\n",
       " (0, 'acellular'),\n",
       " (0, 'acetone'),\n",
       " (0, 'ache'),\n",
       " (0, 'achieve'),\n",
       " (0, 'achieved'),\n",
       " (0, 'achievement'),\n",
       " (0, 'achieves'),\n",
       " (0, 'achieving'),\n",
       " (0, 'aching'),\n",
       " (0, 'acid'),\n",
       " (0, 'acidic'),\n",
       " (0, 'acidity'),\n",
       " (0, 'ack'),\n",
       " (0, 'acknowledge'),\n",
       " (0, 'acknowledged'),\n",
       " (0, 'acknowledgement'),\n",
       " (0, 'acknowledges'),\n",
       " (0, 'aclu'),\n",
       " (0, 'acne'),\n",
       " (0, 'acorn'),\n",
       " (0, 'acosta'),\n",
       " (0, 'acp'),\n",
       " (0, 'acquaintance'),\n",
       " (0, 'acquaintances'),\n",
       " (0, 'acquire'),\n",
       " (0, 'acquired'),\n",
       " (0, 'acquiring'),\n",
       " (0, 'acquisition'),\n",
       " (0, 'acquitted'),\n",
       " (0, 'acres'),\n",
       " (0, 'acronym'),\n",
       " (1, 'across'),\n",
       " (0, 'act'),\n",
       " (0, 'actblue'),\n",
       " (0, 'acted'),\n",
       " (0, 'acting'),\n",
       " (0, 'action'),\n",
       " (0, 'actions'),\n",
       " (0, 'activate'),\n",
       " (0, 'activated'),\n",
       " (0, 'activates'),\n",
       " (0, 'active'),\n",
       " (0, 'actively'),\n",
       " (0, 'activism'),\n",
       " (0, 'activist'),\n",
       " (0, 'activists'),\n",
       " (0, 'activities'),\n",
       " (0, 'activity'),\n",
       " (0, 'actor'),\n",
       " (0, 'actors'),\n",
       " (0, 'actress'),\n",
       " (0, 'acts'),\n",
       " (0, 'actu'),\n",
       " (0, 'actual'),\n",
       " (0, 'actually'),\n",
       " (0, 'acupuncture'),\n",
       " (0, 'acute'),\n",
       " (0, 'ad'),\n",
       " (0, 'ada'),\n",
       " (0, 'adam'),\n",
       " (0, 'adamantly'),\n",
       " (0, 'adams'),\n",
       " (0, 'adaptation'),\n",
       " (0, 'adbance'),\n",
       " (0, 'adcock'),\n",
       " (0, 'add'),\n",
       " (0, 'added'),\n",
       " (0, 'addict'),\n",
       " (0, 'addicted'),\n",
       " (0, 'addiction'),\n",
       " (0, 'addicts'),\n",
       " (0, 'adding'),\n",
       " (0, 'addis'),\n",
       " (0, 'addition'),\n",
       " (0, 'additional'),\n",
       " (0, 'additionally'),\n",
       " (0, 'additives'),\n",
       " (0, 'address'),\n",
       " (0, 'addressed'),\n",
       " (0, 'addresses'),\n",
       " (0, 'addressing'),\n",
       " (0, 'adds'),\n",
       " (0, 'adegbile'),\n",
       " (0, 'adequacy'),\n",
       " (0, 'adequately'),\n",
       " (0, 'adha'),\n",
       " (0, 'adhd'),\n",
       " (0, 'adhere'),\n",
       " (0, 'adhesive'),\n",
       " (0, 'adjourned'),\n",
       " (0, 'adjusted'),\n",
       " (0, 'adjusting'),\n",
       " (0, 'adjustment'),\n",
       " (0, 'adler'),\n",
       " (0, 'adm'),\n",
       " (0, 'admin'),\n",
       " (0, 'administer'),\n",
       " (0, 'administered'),\n",
       " (0, 'administering'),\n",
       " (0, 'administration'),\n",
       " (0, 'administrations'),\n",
       " (0, 'administrative'),\n",
       " (0, 'administrator'),\n",
       " (0, 'administrators'),\n",
       " (0, 'admiral'),\n",
       " (0, 'admire'),\n",
       " (0, 'admired'),\n",
       " (0, 'admission'),\n",
       " (0, 'admissions'),\n",
       " (0, 'admit'),\n",
       " (0, 'admits'),\n",
       " (0, 'admitted'),\n",
       " (0, 'admonished'),\n",
       " (0, 'adolescents'),\n",
       " (0, 'adolf'),\n",
       " (0, 'adopt'),\n",
       " (0, 'adopted'),\n",
       " (0, 'adopting'),\n",
       " (0, 'adoption'),\n",
       " (0, 'adorable'),\n",
       " (0, 'adorned'),\n",
       " (0, 'adorning'),\n",
       " (0, 'adrianna'),\n",
       " (0, 'adrienne'),\n",
       " ...]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(zip(X_train_vect[1].toarray()[0], vect.get_feature_names()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "sm = SMOTE()\n",
    "\n",
    "X_train_res, y_train_res = sm.fit_sample(X_train_vect, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 5954), (1, 5954), (2, 5954)]\n"
     ]
    }
   ],
   "source": [
    "unique, counts = np.unique(y_train_res, return_counts=True)\n",
    "print(list(zip(unique, counts)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7272421901242861"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nb = MultinomialNB()\n",
    "\n",
    "nb.fit(X_train_res, y_train_res)\n",
    "\n",
    "nb.score(X_train_res, y_train_res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_vect = vect.transform(X_test)\n",
    "\n",
    "y_pred = nb.predict(X_test_vect)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 58.44%\n",
      "\n",
      "F1 Score: 48.25\n",
      "\n",
      "Confusion Matrix:\n",
      " [[839 509 106]\n",
      " [341 922  71]\n",
      " [ 94 172  57]]\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy: {:.2f}%\".format(accuracy_score(y_test, y_pred) * 100))\n",
    "print(\"\\nF1 Score: {:.2f}\".format(f1_score(y_test, y_pred, average='macro') * 100))\n",
    "print(\"\\nConfusion Matrix:\\n\", confusion_matrix(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Average accuracy across folds: 57.31%\n",
      "\n",
      "Average F1 score across folds: 46.21\n",
      "\n",
      "Average Confusion Matrix across folds: \n",
      " [[680.6 420.5  89.2]\n",
      " [254.2 710.2  64.9]\n",
      " [ 90.1 143.6  35.7]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import ShuffleSplit\n",
    "\n",
    "vect = CountVectorizer()\n",
    "nb = MultinomialNB()\n",
    "\n",
    "X = train_set.claim\n",
    "y = train_set.label\n",
    "\n",
    "ss = ShuffleSplit(n_splits=10, test_size=0.2)\n",
    "sm = SMOTE()\n",
    "\n",
    "accs = []\n",
    "f1s = []\n",
    "cms = []\n",
    "\n",
    "for train_index, test_index in ss.split(X):\n",
    "    \n",
    "    X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "    y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "    \n",
    "    # Fit vectorizer and transform X train, then transform X test\n",
    "    X_train_vect = vect.fit_transform(X_train)\n",
    "    X_test_vect = vect.transform(X_test)\n",
    "    \n",
    "    # Oversample\n",
    "    X_train_res, y_train_res = sm.fit_sample(X_train_vect, y_train)\n",
    "    \n",
    "    # Fit Naive Bayes on the vectorized X with y train labels, \n",
    "    # then predict new y labels using X test\n",
    "    nb.fit(X_train_res, y_train_res)\n",
    "    y_pred = nb.predict(X_test_vect)\n",
    "    \n",
    "    # Determine test set accuracy and f1 score on this fold using the true y labels and predicted y labels\n",
    "    accs.append(accuracy_score(y_test, y_pred))\n",
    "    f1s.append(f1_score(y_test, y_pred, average='macro'))\n",
    "    cms.append(confusion_matrix(y_test, y_pred))\n",
    "    \n",
    "print(\"\\nAverage accuracy across folds: {:.2f}%\".format(sum(accs) / len(accs) * 100))\n",
    "print(\"\\nAverage F1 score across folds: {:.2f}\".format(sum(f1s) / len(f1s) * 100))\n",
    "print(\"\\nAverage Confusion Matrix across folds: \\n {}\".format(sum(cms) / len(cms)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 1600x900 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(2, 1, sharex=True, figsize=(16,9))\n",
    "\n",
    "acc_scores = [round(a * 100, 1) for a in accs]\n",
    "f1_scores = [round(f * 100, 2) for f in f1s]\n",
    "\n",
    "x1 = np.arange(len(acc_scores))\n",
    "x2 = np.arange(len(f1_scores))\n",
    "\n",
    "ax1.bar(x1, acc_scores)\n",
    "ax2.bar(x2, f1_scores, color='#559ebf')\n",
    "\n",
    "# Place values on top of bars\n",
    "for i, v in enumerate(list(zip(acc_scores, f1_scores))):\n",
    "    ax1.text(i - 0.25, v[0] + 2, str(v[0]) + '%')\n",
    "    ax2.text(i - 0.25, v[1] + 2, str(v[1]))\n",
    "\n",
    "ax1.set_ylabel('Accuracy (%)')\n",
    "ax1.set_title('Naive Bayes')\n",
    "ax1.set_ylim([0, 100])\n",
    "\n",
    "ax2.set_ylabel('F1 Score')\n",
    "ax2.set_xlabel('Runs')\n",
    "ax2.set_ylim([0, 100])\n",
    "\n",
    "sns.despine(bottom=True, left=True)  # Remove the ticks on axes for cleaner presentation\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/alice/projects/fake-news-ai/venv/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/home/alice/projects/fake-news-ai/venv/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:469: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n",
      "/home/alice/projects/fake-news-ai/venv/lib/python3.6/site-packages/sklearn/svm/base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/home/alice/projects/fake-news-ai/venv/lib/python3.6/site-packages/sklearn/ensemble/forest.py:245: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n",
      "/home/alice/projects/fake-news-ai/venv/lib/python3.6/site-packages/sklearn/neural_network/multilayer_perceptron.py:566: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "/home/alice/projects/fake-news-ai/venv/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/home/alice/projects/fake-news-ai/venv/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:469: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n",
      "/home/alice/projects/fake-news-ai/venv/lib/python3.6/site-packages/sklearn/svm/base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/home/alice/projects/fake-news-ai/venv/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/home/alice/projects/fake-news-ai/venv/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:469: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n",
      "/home/alice/projects/fake-news-ai/venv/lib/python3.6/site-packages/sklearn/svm/base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/home/alice/projects/fake-news-ai/venv/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/home/alice/projects/fake-news-ai/venv/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:469: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n",
      "/home/alice/projects/fake-news-ai/venv/lib/python3.6/site-packages/sklearn/svm/base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/home/alice/projects/fake-news-ai/venv/lib/python3.6/site-packages/sklearn/neural_network/multilayer_perceptron.py:566: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "/home/alice/projects/fake-news-ai/venv/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/home/alice/projects/fake-news-ai/venv/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:469: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n",
      "/home/alice/projects/fake-news-ai/venv/lib/python3.6/site-packages/sklearn/svm/base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/home/alice/projects/fake-news-ai/venv/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/home/alice/projects/fake-news-ai/venv/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:469: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n",
      "/home/alice/projects/fake-news-ai/venv/lib/python3.6/site-packages/sklearn/svm/base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/home/alice/projects/fake-news-ai/venv/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/home/alice/projects/fake-news-ai/venv/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:469: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n",
      "/home/alice/projects/fake-news-ai/venv/lib/python3.6/site-packages/sklearn/svm/base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/home/alice/projects/fake-news-ai/venv/lib/python3.6/site-packages/sklearn/neural_network/multilayer_perceptron.py:566: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "/home/alice/projects/fake-news-ai/venv/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/home/alice/projects/fake-news-ai/venv/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:469: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n",
      "/home/alice/projects/fake-news-ai/venv/lib/python3.6/site-packages/sklearn/svm/base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/home/alice/projects/fake-news-ai/venv/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/home/alice/projects/fake-news-ai/venv/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:469: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n",
      "/home/alice/projects/fake-news-ai/venv/lib/python3.6/site-packages/sklearn/svm/base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/home/alice/projects/fake-news-ai/venv/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/home/alice/projects/fake-news-ai/venv/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:469: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n",
      "/home/alice/projects/fake-news-ai/venv/lib/python3.6/site-packages/sklearn/svm/base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/alice/projects/fake-news-ai/venv/lib/python3.6/site-packages/sklearn/neural_network/multilayer_perceptron.py:566: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "/home/alice/projects/fake-news-ai/venv/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/home/alice/projects/fake-news-ai/venv/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:469: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n",
      "/home/alice/projects/fake-news-ai/venv/lib/python3.6/site-packages/sklearn/svm/base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/home/alice/projects/fake-news-ai/venv/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/home/alice/projects/fake-news-ai/venv/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:469: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n",
      "/home/alice/projects/fake-news-ai/venv/lib/python3.6/site-packages/sklearn/svm/base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/home/alice/projects/fake-news-ai/venv/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/home/alice/projects/fake-news-ai/venv/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:469: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n",
      "/home/alice/projects/fake-news-ai/venv/lib/python3.6/site-packages/sklearn/svm/base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/home/alice/projects/fake-news-ai/venv/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/home/alice/projects/fake-news-ai/venv/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:469: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n",
      "/home/alice/projects/fake-news-ai/venv/lib/python3.6/site-packages/sklearn/svm/base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/home/alice/projects/fake-news-ai/venv/lib/python3.6/site-packages/sklearn/neural_network/multilayer_perceptron.py:566: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "/home/alice/projects/fake-news-ai/venv/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/home/alice/projects/fake-news-ai/venv/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:469: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n",
      "/home/alice/projects/fake-news-ai/venv/lib/python3.6/site-packages/sklearn/svm/base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/home/alice/projects/fake-news-ai/venv/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/home/alice/projects/fake-news-ai/venv/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:469: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n",
      "/home/alice/projects/fake-news-ai/venv/lib/python3.6/site-packages/sklearn/svm/base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/home/alice/projects/fake-news-ai/venv/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/home/alice/projects/fake-news-ai/venv/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:469: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n",
      "/home/alice/projects/fake-news-ai/venv/lib/python3.6/site-packages/sklearn/svm/base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/home/alice/projects/fake-news-ai/venv/lib/python3.6/site-packages/sklearn/neural_network/multilayer_perceptron.py:566: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "/home/alice/projects/fake-news-ai/venv/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/home/alice/projects/fake-news-ai/venv/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:469: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n",
      "/home/alice/projects/fake-news-ai/venv/lib/python3.6/site-packages/sklearn/svm/base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/home/alice/projects/fake-news-ai/venv/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/home/alice/projects/fake-news-ai/venv/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:469: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n",
      "/home/alice/projects/fake-news-ai/venv/lib/python3.6/site-packages/sklearn/svm/base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/home/alice/projects/fake-news-ai/venv/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/home/alice/projects/fake-news-ai/venv/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:469: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n",
      "/home/alice/projects/fake-news-ai/venv/lib/python3.6/site-packages/sklearn/svm/base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.linear_model import LogisticRegression, SGDClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "X = train_set.claim\n",
    "y = train_set.label\n",
    "\n",
    "cv = ShuffleSplit(n_splits=20, test_size=0.2)\n",
    "\n",
    "models = [\n",
    "    MultinomialNB(),\n",
    "    BernoulliNB(),\n",
    "    LogisticRegression(),\n",
    "    SGDClassifier(),\n",
    "    LinearSVC(),\n",
    "    RandomForestClassifier(),\n",
    "    MLPClassifier()\n",
    "]\n",
    "\n",
    "sm = SMOTE()\n",
    "\n",
    "# Init a dictionary for storing results of each run for each model\n",
    "results = {\n",
    "    model.__class__.__name__: {\n",
    "        'accuracy': [], \n",
    "        'f1_score': [],\n",
    "        'confusion_matrix': []\n",
    "    } for model in models\n",
    "}\n",
    "\n",
    "for train_index, test_index in cv.split(X):\n",
    "    X_train, X_test  = X.iloc[train_index], X.iloc[test_index]\n",
    "    y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "    \n",
    "    X_train_vect = vect.fit_transform(X_train)    \n",
    "    X_test_vect = vect.transform(X_test)\n",
    "    \n",
    "    X_train_res, y_train_res = sm.fit_sample(X_train_vect, y_train)\n",
    "    \n",
    "    for model in models:\n",
    "        model.fit(X_train_res, y_train_res)\n",
    "        y_pred = model.predict(X_test_vect)\n",
    "        \n",
    "        acc = accuracy_score(y_test, y_pred)\n",
    "        f1 = f1_score(y_test, y_pred, average='macro')\n",
    "        cm = confusion_matrix(y_test, y_pred)\n",
    "        \n",
    "        results[model.__class__.__name__]['accuracy'].append(acc)\n",
    "        results[model.__class__.__name__]['f1_score'].append(f1)\n",
    "        results[model.__class__.__name__]['confusion_matrix'].append(cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MultinomialNB\n",
      "------------------------------\n",
      "        Avg. Accuracy: 57.34%\n",
      "        Avg. F1 Score: 46.57\n",
      "        Avg. Confusion Matrix: \n",
      "        \n",
      "[[686.5  419.45  86.25]\n",
      " [259.4  702.    59.5 ]\n",
      " [ 88.4  148.9   38.6 ]]\n",
      "        \n",
      "BernoulliNB\n",
      "------------------------------\n",
      "        Avg. Accuracy: 47.89%\n",
      "        Avg. F1 Score: 42.40\n",
      "        Avg. Confusion Matrix: \n",
      "        \n",
      "[[483.8  362.3  346.1 ]\n",
      " [196.15 622.2  202.55]\n",
      " [ 63.1  126.9   85.9 ]]\n",
      "        \n",
      "LogisticRegression\n",
      "------------------------------\n",
      "        Avg. Accuracy: 52.49%\n",
      "        Avg. F1 Score: 44.28\n",
      "        Avg. Confusion Matrix: \n",
      "        \n",
      "[[688.75 340.6  162.85]\n",
      " [327.15 560.25 133.5 ]\n",
      " [ 94.   124.4   57.5 ]]\n",
      "        \n",
      "SGDClassifier\n",
      "------------------------------\n",
      "        Avg. Accuracy: 51.21%\n",
      "        Avg. F1 Score: 43.28\n",
      "        Avg. Confusion Matrix: \n",
      "        \n",
      "[[680.9  344.2  167.1 ]\n",
      " [338.35 535.85 146.7 ]\n",
      " [ 94.1  123.85  57.95]]\n",
      "        \n",
      "LinearSVC\n",
      "------------------------------\n",
      "        Avg. Accuracy: 50.02%\n",
      "        Avg. F1 Score: 42.38\n",
      "        Avg. Confusion Matrix: \n",
      "        \n",
      "[[662.6  360.4  169.2 ]\n",
      " [345.9  524.55 150.45]\n",
      " [ 95.3  122.7   57.9 ]]\n",
      "        \n",
      "RandomForestClassifier\n",
      "------------------------------\n",
      "        Avg. Accuracy: 45.40%\n",
      "        Avg. F1 Score: 39.65\n",
      "        Avg. Confusion Matrix: \n",
      "        \n",
      "[[541.45 387.15 263.6 ]\n",
      " [326.7  518.5  175.7 ]\n",
      " [ 87.25 118.5   70.15]]\n",
      "        \n",
      "MLPClassifier\n",
      "------------------------------\n",
      "        Avg. Accuracy: 49.88%\n",
      "        Avg. F1 Score: 42.39\n",
      "        Avg. Confusion Matrix: \n",
      "        \n",
      "[[659.2  373.05 159.95]\n",
      " [351.4  522.8  146.7 ]\n",
      " [ 96.4  120.05  59.45]]\n",
      "        \n"
     ]
    }
   ],
   "source": [
    "for model, d in results.items():\n",
    "    avg_acc = sum(d['accuracy']) / len(d['accuracy']) * 100\n",
    "    avg_f1 = sum(d['f1_score']) / len(d['f1_score']) * 100\n",
    "    avg_cm = sum(d['confusion_matrix']) / len(d['confusion_matrix'])\n",
    "    \n",
    "    slashes = '-' * 30\n",
    "    \n",
    "    s = f\"\"\"{model}\\n{slashes}\n",
    "        Avg. Accuracy: {avg_acc:.2f}%\n",
    "        Avg. F1 Score: {avg_f1:.2f}\n",
    "        Avg. Confusion Matrix: \n",
    "        \\n{avg_cm}\n",
    "        \"\"\"\n",
    "    print(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
